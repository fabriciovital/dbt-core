{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2eaed5-b0cc-4a46-b7ae-3c11c5b41d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 16:30:22,464 - INFO - Starting Delta ingestion...\n",
      "2024-08-20 16:30:22,732 - INFO - Number of rows processed for table sales_countryregioncurrency: 0\n",
      "2024-08-20 16:30:22,733 - INFO - No new data to process for table sales_countryregioncurrency.\n",
      "2024-08-20 16:30:23,212 - INFO - Number of rows processed for table sales_creditcard: 0\n",
      "2024-08-20 16:30:23,213 - INFO - No new data to process for table sales_creditcard.\n",
      "2024-08-20 16:30:23,467 - INFO - Number of rows processed for table sales_currency: 0\n",
      "2024-08-20 16:30:23,468 - INFO - No new data to process for table sales_currency.\n",
      "2024-08-20 16:30:23,736 - INFO - Number of rows processed for table humanresources_department: 0\n",
      "2024-08-20 16:30:23,737 - INFO - No new data to process for table humanresources_department.\n",
      "2024-08-20 16:30:24,003 - INFO - Number of rows processed for table humanresources_employee: 0\n",
      "2024-08-20 16:30:24,004 - INFO - No new data to process for table humanresources_employee.\n",
      "2024-08-20 16:30:24,378 - INFO - Number of rows processed for table sales_salesorderheader: 0\n",
      "2024-08-20 16:30:24,379 - INFO - No new data to process for table sales_salesorderheader.\n",
      "2024-08-20 16:30:24,379 - INFO - Delta ingestion completed!\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as func\n",
    "from configs import configs\n",
    "from functions import functions as F\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "HOST_ADDRESS = os.getenv('HOST_ADDRESS')\n",
    "MINIO_ACCESS_KEY = os.getenv('MINIO_ACCESS_KEY')\n",
    "MINIO_SECRET_KEY = os.getenv('MINIO_SECRET_KEY')\n",
    "USER_POSTGRES = os.getenv('USER_POSTGRES')\n",
    "PASSWORD_POSTGRES = os.getenv('PASSWORD_POSTGRES')\n",
    "\n",
    "def configure_spark():\n",
    "    \"\"\"Configure and return a SparkSession.\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ELT Incremental Landing to Bronze AdventureWorks\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{HOST_ADDRESS}:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def ingest_data():\n",
    "    \"\"\"Ingest data from Landing Zone (Parquet) to Delta Lake in MinIO.\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(\"Starting Delta ingestion...\")\n",
    "\n",
    "    input_prefix_layer_name = configs.prefix_layer_name['0']  # landing layer\n",
    "    table_input_name = configs.lake_path['landing_adventure_works']\n",
    "    output_prefix_layer_name = configs.prefix_layer_name['1']  # bronze layer\n",
    "    storage_output = configs.lake_path['bronze']\n",
    "\n",
    "    for key, value in configs.tables_postgres_adventureworks.items():\n",
    "        table = value\n",
    "        table_name = F.convert_table_name(table)\n",
    "\n",
    "        output_path = f'{storage_output}{output_prefix_layer_name}{table_name}'\n",
    "\n",
    "        try:\n",
    "            # Get the maximum modified date from Delta Lake\n",
    "            max_modified_date_delta = spark.read.format(\"delta\") \\\n",
    "                .load(output_path) \\\n",
    "                .select(func.max(\"modifieddate\") \\\n",
    "                .alias(\"max_modifieddate\")) \\\n",
    "                .collect()[0][\"max_modifieddate\"]\n",
    "\n",
    "            # Read data from Landing Zone (Parquet) and filter based on max_modified_date_delta\n",
    "            input_path = f'{table_input_name}{input_prefix_layer_name}{table_name}'\n",
    "           \n",
    "            df_input_data = spark.read \\\n",
    "                .format(\"parquet\") \\\n",
    "                .load(input_path) \\\n",
    "                .filter(func.col(\"modifieddate\") > max_modified_date_delta)\n",
    "\n",
    "            input_data_count = df_input_data.count()\n",
    "            logging.info(f\"Number of rows processed for table {table_name}: {input_data_count}\")\n",
    "\n",
    "            if input_data_count == 0:\n",
    "                logging.info(f\"No new data to process for table {table_name}.\")\n",
    "                continue\n",
    "\n",
    "            # Add update date metadata and month_key column\n",
    "            df_with_update_date = F.add_metadata(df_input_data)\n",
    "            df_with_month_key = F.add_month_key(df_with_update_date, 'modifieddate')\n",
    "\n",
    "            # Write the new data to Delta Lake\n",
    "            df_with_month_key.write.format(\"delta\").mode(\"append\").partitionBy('month_key').save(output_path)\n",
    "\n",
    "            num_rows_written = df_with_month_key.count()\n",
    "            logging.info(f\"Table {table_name} successfully processed and saved to Delta Lake: {output_path}. {num_rows_written} rows written.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing table {table_name}: {str(e)}\")\n",
    "\n",
    "    logging.info(\"Delta ingestion completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = configure_spark()\n",
    "    ingest_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
