{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736bba4-6223-40d3-bde8-2b11bc0ba225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 10:00:18,255 - INFO - Starting Refinement...\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "from datetime import datetime \n",
    "\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from configs import configs\n",
    "from functions import functions as F\n",
    "\n",
    "\n",
    "def configure_spark():\n",
    "    \"\"\"Configure SparkSession.\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Refinement Incremental Silver to Gold AdventureWorks\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"chapolin\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"mudar@123\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def ingest_data():\n",
    "    \"\"\"Ingest data from AdventureWorks to HDFS.\"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(\"Starting Refinement...\")\n",
    "    \n",
    "    spark = configure_spark()\n",
    "    input_name = configs.prefix_layer_name['2']  # bronze layer\n",
    "    hdfs_input = configs.lake_path['silver']\n",
    "    \n",
    "    output_name = configs.prefix_layer_name['3']  # silver layer\n",
    "    hdfs_output = configs.lake_path['gold']\n",
    "\n",
    "    for table_name, query in configs.tables_gold.items():        \n",
    "        try:\n",
    "            # Obtém a data máxima de modificação do delta da camada gold\n",
    "            max_modified_date_delta = spark.read.format(\"delta\") \\\n",
    "                .load(f'{hdfs_output}{output_name}{table_name}') \\\n",
    "                .select(func.max(\"modifieddate\").alias(\"max_modifieddate\")) \\\n",
    "                .collect()[0][\"max_modifieddate\"]\n",
    "            \n",
    "            # Adiciona o filtro de data na query existente\n",
    "            query_with_filter = f\"\"\"\n",
    "            SELECT * FROM ({query}) AS subquery \n",
    "            WHERE modifieddate > '{max_modified_date_delta}'\n",
    "            \"\"\"\n",
    "\n",
    "            # Executa a query com o filtro aplicado\n",
    "            df_new_data = spark.sql(query_with_filter)\n",
    "            \n",
    "            # Adiciona a coluna de data de atualização\n",
    "            df_with_update_date = F.add_metadata(df_new_data)\n",
    "\n",
    "            # Escreve os dados no Delta Lake\n",
    "            df_with_update_date.write.format(\"delta\").mode(\"append\").partitionBy('month_key').save(f'{hdfs_output}{output_name}{table_name}')\n",
    "\n",
    "            num_rows_written = df_with_update_date.count()\n",
    "            logging.info(f\"Table {table_name} successfully processed and saved to Delta Lake: {hdfs_output}{output_name}{table_name}. {num_rows_written} rows written.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing table {table_name}: {str(e)}\")\n",
    "\n",
    "    logging.info(\"Refinement completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    configure_spark()\n",
    "    ingest_data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
