{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2b44f8-81cf-4677-a3bf-67fb009b1947",
   "metadata": {},
   "source": [
    "# update_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb9343f-751f-4465-88f1-d7d550fb980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd26d97-60c1-4c0c-9bec-2349c8a84c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from configs import configs\n",
    "from functions import functions as F\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d918d03-8a40-4546-9a5f-6af6094230ff",
   "metadata": {},
   "source": [
    "## Import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd55f32-e80f-4b20-be1d-ed2d90467253",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HOST_ADDRESS=os.getenv('HOST_ADDRESS')\n",
    "MINIO_ACCESS_KEY=os.getenv('MINIO_ACCESS_KEY')\n",
    "MINIO_SECRET_KEY=os.getenv('MINIO_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab74f57-3291-47c0-bea6-c2b1bcef5cea",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3f7bf8-e1eb-40a5-9ef1-6ace462fe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_spark():\n",
    "    \"\"\"Configure and return a SparkSession.\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"update_bronze\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{HOST_ADDRESS}:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e68e9-e58a-4858-80bf-1b60908a5bb2",
   "metadata": {},
   "source": [
    "## Function Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a0e9b8-7c0c-4373-903f-783908eccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data():\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(\"Starting ingestion...\")\n",
    "    \n",
    "    input_prefix_layer = configs.prefix_layer_name['1']\n",
    "    storage_input = configs.lake_path['bronze']\n",
    "    \n",
    "    output_prefix_layer = configs.prefix_layer_name['2']\n",
    "    storage_output = configs.lake_path['silver']\n",
    "    \n",
    "    for table_name, query in configs.tables_silver.items():\n",
    "        \n",
    "        input_path = f'{storage_input}{input_prefix_layer}{table_name}'\n",
    "        output_path = f'{storage_output}{output_prefix_layer}{table_name}'\n",
    "        \n",
    "        try:\n",
    "            max_modified_date_destination = spark.read.format(\"delta\") \\\n",
    "                .load(output_path) \\\n",
    "                .select(func.max(\"modifieddate\") \\\n",
    "                .alias(\"max_modifieddate\")) \\\n",
    "                .collect()[0][\"max_modifieddate\"]\n",
    "            #print(max_modified_date_destination)\n",
    "            \n",
    "            df_input_data = spark.read \\\n",
    "                .format(\"delta\") \\\n",
    "                .load(input_path) \\\n",
    "                .filter(func.col(\"modifieddate\") > func.lit(max_modified_date_destination))\n",
    "            \n",
    "            input_data_count = df_input_data.count()\n",
    "            logging.info(f\"Number os rows processed for table {table_name}: {input_data_count}\")\n",
    "            \n",
    "            if input_data_count == 0:\n",
    "                logging.info(f\"No new data to process for table {table_name}.\")\n",
    "                continue\n",
    "                      \n",
    "            df_with_update_date = F.add_metadata(df_input_data)\n",
    "            df_with_month_key = F.add_month_key(df_with_update_date, 'modifieddate')\n",
    "            \n",
    "            df_with_month_key.write.format(\"delta\").mode(\"append\").partitionBy(\"month_key\").save(output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing table {table_name}: {str(e)}.\")\n",
    "    \n",
    "    logging.info(\"Ingestion completed!\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b975f7-dcd0-45a7-8407-21cf6ea8ff9b",
   "metadata": {},
   "source": [
    "## Function Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5c73ef-2951-4223-bbc6-2bdff16d647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 14:13:21,353 - INFO - Starting ingestion...\n",
      "2024-08-21 14:13:21,755 - INFO - Number os rows processed for table sales_countryregioncurrency: 0\n",
      "2024-08-21 14:13:21,756 - INFO - No new data to process for table sales_countryregioncurrency.\n",
      "2024-08-21 14:13:22,203 - INFO - Number os rows processed for table humanresources_department: 0\n",
      "2024-08-21 14:13:22,204 - INFO - No new data to process for table humanresources_department.\n",
      "2024-08-21 14:13:22,631 - INFO - Number os rows processed for table humanresources_employee: 0\n",
      "2024-08-21 14:13:22,632 - INFO - No new data to process for table humanresources_employee.\n",
      "2024-08-21 14:13:23,156 - INFO - Number os rows processed for table sales_salesorderheader: 0\n",
      "2024-08-21 14:13:23,157 - INFO - No new data to process for table sales_salesorderheader.\n",
      "2024-08-21 14:13:23,157 - INFO - Ingestion completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = configure_spark()\n",
    "    ingest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d3f2e7-d553-4fed-ba43-4e33007ef2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+-------------------+--------------------+---------+\n",
      "|departmentid|                name|           groupname|       modifieddate|         last_update|month_key|\n",
      "+------------+--------------------+--------------------+-------------------+--------------------+---------+\n",
      "|           1|         Engineering|Research and Deve...|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|           2|         Tool Design|Research and Deve...|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|           3|               Sales| Sales and Marketing|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|           4|           Marketing| Sales and Marketing|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|           5|          Purchasing|Inventory Management|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|           6|Research and Deve...|Research and Deve...|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|           7|          Production|       Manufacturing|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|           8|  Production Control|       Manufacturing|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|           9|     Human Resources|Executive General...|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|          10|             Finance|Executive General...|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|          11|Information Services|Executive General...|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|          12|    Document Control|   Quality Assurance|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|          13|   Quality Assurance|   Quality Assurance|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|          14|Facilities and Ma...|Executive General...|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|          15|Shipping and Rece...|Inventory Management|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|          16|           Executive|Executive General...|2008-04-30 00:00:00|2024-08-21 10:13:...|   200804|\n",
      "|          17|Business Intellig...|          Technology|2024-04-30 00:00:00|2024-08-21 13:55:...|   202404|\n",
      "|          18|       Data Engineer|          Technology|2024-05-02 00:00:00|2024-08-21 14:13:...|   202405|\n",
      "+------------+--------------------+--------------------+-------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load('s3a://silver/adventure_works/silver_humanresources_department').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
