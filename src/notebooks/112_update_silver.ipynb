{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809fe53c-20be-4c63-8ad4-850c7a05ba39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 09:59:57,187 - INFO - Starting Processing......\n",
      "2024-08-21 09:59:58,184 - INFO - Table sales_countryregioncurrency successfully processed and saved to Delta Lake: s3a://silver/adventure_works/silver_sales_countryregioncurrency. 0 rows written.\n",
      "2024-08-21 09:59:59,280 - INFO - Table humanresources_department successfully processed and saved to Delta Lake: s3a://silver/adventure_works/silver_humanresources_department. 0 rows written.\n",
      "2024-08-21 10:00:00,583 - INFO - Table humanresources_employee successfully processed and saved to Delta Lake: s3a://silver/adventure_works/silver_humanresources_employee. 0 rows written.\n",
      "2024-08-21 10:00:02,006 - INFO - Table sales_salesorderheader successfully processed and saved to Delta Lake: s3a://silver/adventure_works/silver_sales_salesorderheader. 0 rows written.\n",
      "2024-08-21 10:00:02,007 - INFO - Processing completed!\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "from datetime import datetime \n",
    "\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "from configs import configs\n",
    "from functions import functions as F\n",
    "\n",
    "def configure_spark():\n",
    "    \"\"\"Configure SparkSession.\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "            .appName(\"Process Incremental Bronze to Silver AdventureWorks\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", \"chapolin\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", \"mudar@123\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "            .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def ingest_data():\n",
    "    \"\"\"Ingest data from AdventureWorks to HDFS.\"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    logging.info(\"Starting Processing......\")\n",
    "    \n",
    "    spark = configure_spark()\n",
    "    input_name = configs.prefix_layer_name['1']  # bronze layer\n",
    "    hdfs_input = configs.lake_path['bronze']\n",
    "    \n",
    "    output_name = configs.prefix_layer_name['2']  # silver layer\n",
    "    hdfs_output = configs.lake_path['silver']\n",
    "\n",
    "    for table_name, query in configs.tables_silver.items():        \n",
    "        try:\n",
    "            df_source = spark.read.format(\"delta\").load(f'{hdfs_input}{input_name}{table_name}')\n",
    "\n",
    "            df_delta = spark.read.format(\"delta\").load(f'{hdfs_output}{output_name}{table_name}')\n",
    "            \n",
    "            #Get max date from destination\n",
    "            max_modified_date_delta = spark.read.format(\"delta\") \\\n",
    "                .load(f'{hdfs_output}{output_name}{table_name}') \\\n",
    "                .select(func.max(\"modifieddate\").alias(\"max_modifieddate\")) \\\n",
    "                .collect()[0][\"max_modifieddate\"]\n",
    "            \n",
    "            df_new_data = spark.read.format(\"delta\") \\\n",
    "                .load(f'{hdfs_input}{input_name}{table_name}') \\\n",
    "                .filter(func.col(\"modifieddate\") > lit(max_modified_date_delta))\n",
    "         \n",
    "            df_with_update_date = F.add_metadata(df_new_data)\n",
    "\n",
    "            df_with_update_date.write.format(\"delta\").mode(\"append\").partitionBy('month_key').option(\"mergeSchema\", \"true\").save(f'{hdfs_output}{output_name}{table_name}')\n",
    "\n",
    "            num_rows_written = df_with_update_date.count()\n",
    "            logging.info(f\"Table {table_name} successfully processed and saved to Delta Lake: {hdfs_output}{output_name}{table_name}. {num_rows_written} rows written.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing table {table_name}: {str(e)}\")\n",
    "\n",
    "    logging.info(\"Processing completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    configure_spark()\n",
    "    ingest_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
