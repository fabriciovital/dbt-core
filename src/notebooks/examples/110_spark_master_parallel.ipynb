{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0b4540-b6ce-4322-ae3b-0f2e413d1b79",
   "metadata": {},
   "source": [
    "# Sample Spark Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a4cf80-00d8-43b6-b513-5dabab94651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from configs import configs\n",
    "from functions import functions as F\n",
    "from pyspark.sql import functions as func\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HOST_ADDRESS=os.getenv('HOST_ADDRESS')\n",
    "MINIO_ACCESS_KEY=os.getenv('MINIO_ACCESS_KEY')\n",
    "MINIO_SECRET_KEY=os.getenv('MINIO_SECRET_KEY')\n",
    "\n",
    "USER_POSTGRES=os.getenv('USER_POSTGRES')\n",
    "PASSWORD_POSTGRES=os.getenv('PASSWORD_POSTGRES')\n",
    "\n",
    "def configure_spark():\n",
    "    \"\"\"Configure and return a SparkSession.\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Sample Spark Master Parallel\") \\\n",
    "        .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{HOST_ADDRESS}:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0dc29a9-781d-4352-8f5c-fe5d316e7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(spark, table, table_name, storage_output, output_prefix_layer):\n",
    "    output_path = f'{storage_output}{output_prefix_layer}{table_name}'\n",
    "    \n",
    "    try:\n",
    "        max_modified_date_destination = spark.read.format(\"parquet\") \\\n",
    "            .load(output_path) \\\n",
    "            .select(func.max(\"modifieddate\") \\\n",
    "            .alias(\"max_modifieddate\")) \\\n",
    "            .collect()[0][\"max_modifieddate\"]\n",
    "        #print(max_modified_date_destination)\n",
    "\n",
    "        query = f\"\"\" select * from {table} where modifieddate > '{max_modified_date_destination}'\"\"\"\n",
    "\n",
    "        df_input_data = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", f\"jdbc:postgresql://{HOST_ADDRESS}:5435/Adventureworks\") \\\n",
    "            .option(\"user\", USER_POSTGRES) \\\n",
    "            .option(\"dbtable\", f\"({query}) as filtered\") \\\n",
    "            .option(\"password\", PASSWORD_POSTGRES) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load()\n",
    "\n",
    "        input_data_count = df_input_data.count()\n",
    "        logging.info(f\"Number os rows processed for table {table_name}: {input_data_count}\")\n",
    "\n",
    "        if input_data_count == 0:\n",
    "            logging.info(f\"No new data to process for table {table_name}.\")\n",
    "            return\n",
    "        \n",
    "        df_with_update_date = F.add_metadata(df_input_data)\n",
    "        df_with_month_key = F.add_month_key(df_with_update_date, 'modifieddate')\n",
    "\n",
    "        df_with_month_key.write.format(\"delta\").mode(\"append\").partitionBy(\"month_key\").save(output_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing table {table_name}: {str(e)}.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d20e290-004f-456a-b67a-b69247e1dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 14:57:31,608 - INFO - Starting ingestion...\n",
      "2024-08-22 14:57:34,131 - INFO - Number os rows processed for table humanresources_department: 0\n",
      "2024-08-22 14:57:34,136 - INFO - No new data to process for table humanresources_department.\n",
      "2024-08-22 14:57:34,283 - INFO - Number os rows processed for table humanresources_employee: 0\n",
      "2024-08-22 14:57:34,290 - INFO - No new data to process for table humanresources_employee.\n",
      "2024-08-22 14:57:34,313 - INFO - Number os rows processed for table sales_currency: 0\n",
      "2024-08-22 14:57:34,315 - INFO - Number os rows processed for table sales_countryregioncurrency: 0\n",
      "2024-08-22 14:57:34,320 - INFO - No new data to process for table sales_countryregioncurrency.\n",
      "2024-08-22 14:57:34,319 - INFO - No new data to process for table sales_currency.\n",
      "2024-08-22 14:57:34,332 - INFO - Number os rows processed for table sales_creditcard: 0\n",
      "2024-08-22 14:57:34,334 - INFO - No new data to process for table sales_creditcard.\n",
      "2024-08-22 14:57:34,400 - INFO - Number os rows processed for table sales_salesorderheader: 0\n",
      "2024-08-22 14:57:34,401 - INFO - No new data to process for table sales_salesorderheader.\n",
      "2024-08-22 14:57:34,402 - INFO - Closing down clientserver connection\n",
      "2024-08-22 14:57:34,402 - INFO - Closing down clientserver connection\n",
      "2024-08-22 14:57:34,403 - INFO - Closing down clientserver connection\n",
      "2024-08-22 14:57:34,403 - INFO - Closing down clientserver connection\n",
      "2024-08-22 14:57:34,404 - INFO - Closing down clientserver connection\n",
      "2024-08-22 14:57:34,404 - INFO - Closing down clientserver connection\n",
      "2024-08-22 14:57:34,413 - INFO - Parallel ingestion completed!\n"
     ]
    }
   ],
   "source": [
    "def ingest_data():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    logging.info(\"Starting ingestion...\")\n",
    "    \n",
    "    output_prefix_layer = configs.prefix_layer_name['0']\n",
    "    storage_output = configs.lake_path['landing_adventure_works']\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        futures = []\n",
    "        for key, value in configs.tables_postgres_adventureworks.items():\n",
    "            table = value\n",
    "            table_name = F.convert_table_name(table)\n",
    "            futures.append(executor.submit(process_table, spark, table, table_name, storage_output, output_prefix_layer))\n",
    " \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f'Error in one od the parallel task : {str(e)}.')\n",
    "    \n",
    "    logging.info(\"Parallel ingestion completed!\") \n",
    "   \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    spark = configure_spark()\n",
    "    ingest_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
