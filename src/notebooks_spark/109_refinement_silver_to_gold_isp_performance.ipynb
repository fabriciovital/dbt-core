{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2b44f8-81cf-4677-a3bf-67fb009b1947",
   "metadata": {},
   "source": [
    "# process_bronze_to_silver_isp_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd26d97-60c1-4c0c-9bec-2349c8a84c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from configs import configs\n",
    "from functions import functions as F\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d918d03-8a40-4546-9a5f-6af6094230ff",
   "metadata": {},
   "source": [
    "## Import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd55f32-e80f-4b20-be1d-ed2d90467253",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HOST_ADDRESS=os.getenv('HOST_ADDRESS')\n",
    "MINIO_ACCESS_KEY=os.getenv('MINIO_ACCESS_KEY')\n",
    "MINIO_SECRET_KEY=os.getenv('MINIO_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72167ff-682b-4436-a0d1-295756ce7afc",
   "metadata": {},
   "source": [
    "## Function process table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb64e141-973f-4c6f-a73b-af6577466b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(spark, query_input, output_path):\n",
    "    try:\n",
    "        df_input_data = spark.sql(query_input)\n",
    "        df_with_update_date = F.add_metadata(df_input_data)\n",
    "        df_with_update_date.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy('month_key') \\\n",
    "            .save(output_path)\n",
    "        logging.info(f\"query '{query_input}' successfully processed and saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing query '{query_input}': {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab74f57-3291-47c0-bea6-c2b1bcef5cea",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eec531d-8d49-4d76-9155-2b4c41164e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "            .appName(\"process_bronze_to_gold_isp_performance\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", f\"http://{HOST_ADDRESS}:9000\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "            .config(\"hive.metastore.uris\", \"thrift://metastore:9083\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db9c99-7633-4d6a-b924-1aedef02d91c",
   "metadata": {},
   "source": [
    "## Log configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7567f5d1-2aec-4d4d-8bd4-a307ec95feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 22:23:47,889 - INFO - Starting processing from bronze to gold...\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting processing from bronze to gold...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e468c0-7950-49fc-9112-b65960b519a3",
   "metadata": {},
   "source": [
    "## Path configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca49cdd-c4cb-49e8-b311-b16e7c0046e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prefix_layer_name = configs.prefix_layer_name['2']  # bronze layer\n",
    "input_path = configs.lake_path['silver']\n",
    "\n",
    "output_prefix_layer_name = configs.prefix_layer_name['3']  # silver layer\n",
    "output_path = configs.lake_path['gold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d4262-40e9-4722-a7e4-eb77344dd482",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59373a4f-a244-46c4-9d56-9200f10ff55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 22:24:04,080 - INFO - query '\n",
      "SELECT\n",
      "    t1.mensagem_resposta,\n",
      "    t1.data_hora_analise,\n",
      "    t1.data_hora_encaminhado,\n",
      "    t1.data_hora_assumido,\n",
      "    t1.data_hora_execucao,\n",
      "    t1.id_contrato_kit,\n",
      "    t1.preview,\n",
      "    t1.data_agenda_final,\n",
      "    t1.id,\n",
      "    t1.tipo,\n",
      "    t1.id_filial,\n",
      "    t2.fantasia,\n",
      "    t1.id_wfl_tarefa,\n",
      "    t1.status_sla,\n",
      "    t1.data_abertura,\n",
      "    t1.ano_abertura,\n",
      "    t1.ano_mes_abertura,\n",
      "    t1.mes_abertura,\n",
      "    t1.trimestre_abertura,\n",
      "    t1.semana_do_ano_abertura,\n",
      "    t1.semana_do_mes_abertura,\n",
      "    t1.dia_da_semana_abertura,\n",
      "    t1.dia_do_mes_abertura,\n",
      "    t1.hora_abertura,\n",
      "    t1.periodo_horario_abertura,\n",
      "    t1.melhor_horario_agenda,\n",
      "    t1.liberado,\n",
      "    t1.status,\n",
      "    t1.id_cliente,\n",
      "    t1.id_assunto,\n",
      "    t3.assunto,\n",
      "    t1.setor,\n",
      "    t1.id_cidade,\n",
      "    t1.id_tecnico,\n",
      "    t4.funcionario,\n",
      "    t1.prioridade,\n",
      "    t1.mensagem,\n",
      "    t1.protocolo,\n",
      "    t1.endereco,\n",
      "    t1.complemento,\n",
      "    t1.id_condominio,\n",
      "    t1.bloco,\n",
      "    t1.apartamento,\n",
      "    t1.latitude,\n",
      "    t1.bairro,\n",
      "    t1.longitude,\n",
      "    t1.referencia,\n",
      "    t1.impresso,\n",
      "    t1.data_inicio,\n",
      "    t1.data_agenda,\n",
      "    t1.data_final,\n",
      "    t1.data_fechamento,\n",
      "    t1.id_wfl_param_os,\n",
      "    t1.valor_total_comissao,\n",
      "    t1.valor_total,\n",
      "    t1.valor_outras_despesas,\n",
      "    t1.idx,\n",
      "    t1.id_su_diagnostico,\n",
      "    t1.gera_comissao,\n",
      "    t1.id_estrutura,\n",
      "    t1.id_login,\n",
      "    t1.valor_unit_comissao,\n",
      "    t1.data_prazo_limite,\n",
      "    t1.data_reservada,\n",
      "    t1.id_ticket,\n",
      "    t1.origem_endereco,\n",
      "    t1.justificativa_sla_atrasado,\n",
      "    t1.origem_endereco_estrutura,\n",
      "    t1.data_reagendar,\n",
      "    t1.data_prev_final,\n",
      "    t1.origem_cadastro,\n",
      "    t1.ultima_atualizacao,\n",
      "    t1.last_update,\n",
      "    t1.month_key\n",
      "FROM\n",
      "     delta.`s3a://silver/isp_performance/silver_ordem_servico_aberto` t1\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_filial` t2 ON (t2.id = t1.id_filial)\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_assunto` t3 ON (t3.id = t1.id_assunto)\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_colaboradores` t4 ON (t4.id = t1.id_tecnico)\n",
      "    ' successfully processed and saved to s3a://gold/isp_performance/gold_ordem_servico_aberto\n",
      "2024-11-02 23:12:36,273 - ERROR - Error processing query '\n",
      "SELECT\n",
      "    t1.mensagem_resposta,\n",
      "    t1.data_hora_analise,\n",
      "    t1.data_hora_encaminhado,\n",
      "    t1.data_hora_assumido,\n",
      "    t1.data_hora_execucao,\n",
      "    t1.id_contrato_kit,\n",
      "    t1.preview,\n",
      "    t1.data_agenda_final,\n",
      "    t1.id,\n",
      "    t1.tipo,\n",
      "    t1.id_filial,\n",
      "    t2.fantasia,\n",
      "    t1.id_wfl_tarefa,\n",
      "    t1.status_sla,\n",
      "    t1.data_abertura,\n",
      "    t1.ano_abertura,\n",
      "    t1.ano_mes_abertura,\n",
      "    t1.mes_abertura,\n",
      "    t1.trimestre_abertura,\n",
      "    t1.semana_do_ano_abertura,\n",
      "    t1.semana_do_mes_abertura,\n",
      "    t1.dia_da_semana_abertura,\n",
      "    t1.dia_do_mes_abertura,\n",
      "    t1.hora_abertura,\n",
      "    t1.periodo_horario_abertura,\n",
      "    t1.melhor_horario_agenda,\n",
      "    t1.liberado,\n",
      "    t1.status,\n",
      "    t1.id_cliente,\n",
      "    t1.id_assunto,\n",
      "    t3.assunto,\n",
      "    t1.setor,\n",
      "    t1.id_cidade,\n",
      "    t1.id_tecnico,\n",
      "    t4.funcionario,\n",
      "    t1.prioridade,\n",
      "    t1.mensagem,\n",
      "    t1.protocolo,\n",
      "    t1.endereco,\n",
      "    t1.complemento,\n",
      "    t1.id_condominio,\n",
      "    t1.bloco,\n",
      "    t1.apartamento,\n",
      "    t1.latitude,\n",
      "    t1.bairro,\n",
      "    t1.longitude,\n",
      "    t1.referencia,\n",
      "    t1.impresso,\n",
      "    t1.data_inicio,\n",
      "    t1.data_agenda,\n",
      "    t1.data_final,\n",
      "    t1.data_fechamento,\n",
      "    t1.ano_fechamento,\n",
      "    t1.ano_mes_fechamento,\n",
      "    t1.mes_fechamento,\n",
      "    t1.trimestre_fechamento,\n",
      "    t1.semana_do_ano,\n",
      "    t1.semana_do_mes_fechamento,\n",
      "    t1.dia_da_semana_fechamento,\n",
      "    t1.dia_do_mes_fechamento,\n",
      "    t1.hora_fechamento,\n",
      "    t1.periodo_horario_fechamento,\n",
      "    t1.id_wfl_param_os,\n",
      "    t1.valor_total_comissao,\n",
      "    t1.valor_total,\n",
      "    t1.valor_outras_despesas,\n",
      "    t1.idx,\n",
      "    t1.id_su_diagnostico,\n",
      "    t1.gera_comissao,\n",
      "    t1.id_estrutura,\n",
      "    t1.id_login,\n",
      "    t1.valor_unit_comissao,\n",
      "    t1.data_prazo_limite,\n",
      "    t1.data_reservada,\n",
      "    t1.id_ticket,\n",
      "    t1.origem_endereco,\n",
      "    t1.justificativa_sla_atrasado,\n",
      "    t1.origem_endereco_estrutura,\n",
      "    t1.data_reagendar,\n",
      "    t1.data_prev_final,\n",
      "    t1.origem_cadastro,\n",
      "    t1.ultima_atualizacao,\n",
      "    t1.last_update,\n",
      "    t1.month_key\n",
      "FROM\n",
      "     delta.`s3a://silver/isp_performance/silver_ordem_servico_fechado` t1\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_filial` t2 ON (t2.id = t1.id_filial)\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_assunto` t3 ON (t3.id = t1.id_assunto)\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_colaboradores` t4 ON (t4.id = t1.id_tecnico)\n",
      "    ': An error occurred while calling o72.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:397)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:352)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:327)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:101)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:212)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:209)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:101)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:324)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:98)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:91)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:252)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:91)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:159)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:303)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 201 in stage 73.0 failed 1 times, most recent failure: Lost task 201.0 in stage 73.0 (TID 1256) (spark-master executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:655)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.nio.file.AccessDeniedException: s3a://gold/isp_performance/gold_ordem_servico_fechado/month_key=202206/part-00201-43d981f8-6fc1-43ba-aa39-113e1a2380b8.c000.snappy.parquet: getFileStatus on s3a://gold/isp_performance/gold_ordem_servico_fechado/month_key=202206/part-00201-43d981f8-6fc1-43ba-aa39-113e1a2380b8.c000.snappy.parquet: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 180449AB02F4F371; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:403 Forbidden\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n",
      "\tat org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker.getFileSize(BasicWriteStatsTracker.scala:89)\n",
      "\tat org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker.getFileSize(BasicWriteStatsTracker.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker.updateFileStats(BasicWriteStatsTracker.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker.closeFile(BasicWriteStatsTracker.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$releaseCurrentWriter$1(FileFormatDataWriter.scala:65)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$releaseCurrentWriter$1$adapted(FileFormatDataWriter.scala:65)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:65)\n",
      "\tat org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.renewCurrentWriter(FileFormatDataWriter.scala:267)\n",
      "\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:365)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\t... 9 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 180449AB02F4F371; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
      "\t... 26 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\t... 59 more\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:655)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: java.nio.file.AccessDeniedException: s3a://gold/isp_performance/gold_ordem_servico_fechado/month_key=202206/part-00201-43d981f8-6fc1-43ba-aa39-113e1a2380b8.c000.snappy.parquet: getFileStatus on s3a://gold/isp_performance/gold_ordem_servico_fechado/month_key=202206/part-00201-43d981f8-6fc1-43ba-aa39-113e1a2380b8.c000.snappy.parquet: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 180449AB02F4F371; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8:403 Forbidden\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n",
      "\tat org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker.getFileSize(BasicWriteStatsTracker.scala:89)\n",
      "\tat org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker.getFileSize(BasicWriteStatsTracker.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker.updateFileStats(BasicWriteStatsTracker.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker.closeFile(BasicWriteStatsTracker.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$releaseCurrentWriter$1(FileFormatDataWriter.scala:65)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.$anonfun$releaseCurrentWriter$1$adapted(FileFormatDataWriter.scala:65)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:65)\n",
      "\tat org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.renewCurrentWriter(FileFormatDataWriter.scala:267)\n",
      "\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:365)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\t... 9 more\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 180449AB02F4F371; S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8; Proxy: null), S3 Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
      "\t... 26 more\n",
      "\n",
      "2024-11-02 23:12:50,613 - INFO - query '\n",
      "SELECT\n",
      "    t1.ano_abertura,\n",
      "    t1.ano_mes_abertura,\n",
      "    t1.mes_abertura,\n",
      "    t1.trimestre_abertura,\n",
      "    t1.semana_do_ano_abertura,\n",
      "    t1.semana_do_mes_abertura,\n",
      "    t1.dia_da_semana_abertura,\n",
      "    t1.dia_do_mes_abertura,\n",
      "    t1.hora_abertura,\n",
      "    t1.periodo_horario_abertura,\n",
      "    t1.id_filial,\n",
      "    t2.fantasia,\n",
      "    t1.setor,\n",
      "    t1.id_assunto,\n",
      "    t3.assunto,\n",
      "    t1.id_tecnico,\n",
      "    t4.funcionario,\n",
      "    t1.status,\n",
      "    COUNT(DISTINCT(t1.id)) AS total_ordem_aberta,\n",
      "    t1.month_key\n",
      "FROM\n",
      "    delta.`s3a://silver/isp_performance/silver_ordem_servico_aberto` t1\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_filial` t2 ON (t2.id = t1.id_filial)\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_assunto` t3 ON (t3.id = t1.id_assunto)\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_colaboradores` t4 ON (t4.id = t1.id_tecnico)\n",
      "GROUP BY\n",
      "    t1.ano_abertura,\n",
      "    t1.ano_mes_abertura,\n",
      "    t1.mes_abertura,\n",
      "    t1.trimestre_abertura,\n",
      "    t1.semana_do_ano_abertura,\n",
      "    t1.semana_do_mes_abertura,\n",
      "    t1.dia_da_semana_abertura,\n",
      "    t1.dia_do_mes_abertura,\n",
      "    t1.hora_abertura,\n",
      "    t1.periodo_horario_abertura,\n",
      "    t1.id_filial,\n",
      "    t2.fantasia,\n",
      "    t1.setor,\n",
      "    t1.id_assunto,\n",
      "    t3.assunto,\n",
      "    t1.id_tecnico,\n",
      "    t4.funcionario,\n",
      "    t1.status,\n",
      "    t1.month_key\n",
      "    ' successfully processed and saved to s3a://gold/isp_performance/gold_ordem_servico_aberto_resumo_situacao\n",
      "2024-11-02 23:13:33,326 - INFO - query '\n",
      "SELECT\n",
      "    t1.ano_abertura,\n",
      "    t1.ano_mes_abertura,\n",
      "    t1.mes_abertura,\n",
      "    t1.trimestre_abertura,\n",
      "    t1.semana_do_ano_abertura,\n",
      "    t1.semana_do_mes_abertura,\n",
      "    t1.dia_da_semana_abertura,\n",
      "    t1.dia_do_mes_abertura,\n",
      "    t1.hora_abertura,\n",
      "    t1.periodo_horario_abertura,\n",
      "    t1.ano_fechamento,\n",
      "    t1.ano_mes_fechamento,\n",
      "    t1.mes_fechamento,\n",
      "    t1.trimestre_fechamento,\n",
      "    t1.semana_do_ano,\n",
      "    t1.semana_do_mes_fechamento,\n",
      "    t1.dia_da_semana_fechamento,\n",
      "    t1.dia_do_mes_fechamento,\n",
      "    t1.hora_fechamento,\n",
      "    t1.periodo_horario_fechamento,\n",
      "    t1.id_filial,\n",
      "    t2.fantasia,\n",
      "    t1.setor,\n",
      "    t1.id_assunto,\n",
      "    t3.assunto,\n",
      "    t1.id_tecnico,\n",
      "    t4.funcionario,\n",
      "    t1.status,\n",
      "    COUNT(DISTINCT(t1.id)) AS total_ordem_fechado,\n",
      "    t1.month_key\n",
      "FROM\n",
      "    delta.`s3a://silver/isp_performance/silver_ordem_servico_fechado` t1\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_filial` t2 ON (t2.id = t1.id_filial)\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_assunto` t3 ON (t3.id = t1.id_assunto)\n",
      "LEFT JOIN delta.`s3a://silver/isp_performance/silver_dim_colaboradores` t4 ON (t4.id = t1.id_tecnico)\n",
      "GROUP BY\n",
      "    t1.ano_abertura,\n",
      "    t1.ano_mes_abertura,\n",
      "    t1.mes_abertura,\n",
      "    t1.trimestre_abertura,\n",
      "    t1.semana_do_ano_abertura,\n",
      "    t1.semana_do_mes_abertura,\n",
      "    t1.dia_da_semana_abertura,\n",
      "    t1.dia_do_mes_abertura,\n",
      "    t1.hora_abertura,\n",
      "    t1.periodo_horario_abertura,\n",
      "    t1.ano_fechamento,\n",
      "    t1.ano_mes_fechamento,\n",
      "    t1.mes_fechamento,\n",
      "    t1.trimestre_fechamento,\n",
      "    t1.semana_do_ano,\n",
      "    t1.semana_do_mes_fechamento,\n",
      "    t1.dia_da_semana_fechamento,\n",
      "    t1.dia_do_mes_fechamento,\n",
      "    t1.hora_fechamento,\n",
      "    t1.periodo_horario_fechamento,\n",
      "    t1.id_filial,\n",
      "    t2.fantasia,\n",
      "    t1.setor,\n",
      "    t1.id_assunto,\n",
      "    t3.assunto,\n",
      "    t1.id_tecnico,\n",
      "    t4.funcionario,\n",
      "    t1.status,\n",
      "    t1.month_key\n",
      "    ' successfully processed and saved to s3a://gold/isp_performance/gold_ordem_servico_fechado_resumo_situacao\n",
      "2024-11-02 23:13:33,327 - INFO - Process to gold completed!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for table_name, query_input in configs.tables_gold.items():\n",
    "        table_name = F.convert_table_name(table_name)\n",
    "        \n",
    "        query_input = F.get_query(table_name, input_path, input_prefix_layer_name, configs.tables_gold)        \n",
    "        \n",
    "        storage_output = f'{output_path}{output_prefix_layer_name}{table_name}'\n",
    "        \n",
    "        process_table(spark, query_input, storage_output)\n",
    "        \n",
    "    logging.info(\"Process to gold completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f'Error processing table: {str(e)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
